{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7ee29a",
   "metadata": {},
   "source": [
    "## How a trulens eval function works on llamaindex platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63d449da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cea90dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    }
   ],
   "source": [
    "# Initializing trulens session to track evaluations\n",
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "# session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14c85e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n",
    "file_path = \"data_llama/paul_graham_essay.txt\"\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01575af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Setting the config\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 16\n",
    "Settings.llm = OpenAI()\n",
    "#Loading the data\n",
    "documents = SimpleDirectoryReader(\"data_llama\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "#Creating a query engine\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "# Validate if the query engine is working\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e26f8d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.calls[-1].rets.source_nodes[:].node.text.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input context will be set to __record__.calls[-1].rets.source_nodes[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "## Reference documents\n",
    "# https://www.trulens.org/getting_started/quickstarts/llama_index_quickstart/\n",
    "import numpy as np\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.core import Feedback\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "provider = OpenAI(model_engine=\"gpt-4.1-mini\")\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "\n",
    "context = TruLlama.select_context(query_engine)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(context.collect())  # collect context chunks into a list\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118177d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the app\n",
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_name=\"LlamaIndex_App\",\n",
    "    app_version=\"base\",\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9ff2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    query_engine.query(\"What kind of writing did he do as a beginner?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eebb6cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the first computer he encountered?\n",
      "Response: IBM 1401\n",
      "---\n",
      "Question: What did Paul Graham initially plan to study in college?\n",
      "Response: Paul Graham initially planned to study philosophy in college.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "eval_questions = [\"What was the first computer he encountered?\", 'What did Paul Graham initially plan to study in college?']\n",
    "\n",
    "with tru_query_engine_recorder as recording:\n",
    "    for question in eval_questions:\n",
    "        response = query_engine.query(question)\n",
    "        print(f\"Question: {question}\\nResponse: {response.response}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f5ee11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Dashboard already running at path:   Local URL: http://localhost:49896\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens_eval.feedback import Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60382480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 16\n",
    "Settings.llm = OpenAI()\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data_llama\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475149c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.core import Feedback\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# Initialize provider class\n",
    "provider = OpenAI(model_engine=\"gpt-4.1-mini\")\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "\n",
    "context = TruLlama.select_context(query_engine)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
    "    )\n",
    "    .on(context.collect())  # collect context chunks into a list\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    "    )\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21511c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(\n",
    "    query_engine,\n",
    "    app_name=\"LlamaIndex_App\",\n",
    "    app_version=\"base\",\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510231d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as context manager\n",
    "with tru_query_engine_recorder as recording:\n",
    "    query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5db55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard.display import get_feedback_result\n",
    "\n",
    "last_record = recording.records[-1]\n",
    "get_feedback_result(last_record, \"Context Relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.llamaindex.guardrails import WithFeedbackFilterNodes\n",
    "\n",
    "guardrail_provider = OpenAI(model_engine=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e16a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: feedback function used for guardrail must only return a score, not also reasons\n",
    "f_context_relevance_score = Feedback(guardrail_provider.context_relevance, name=\"Context Relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_query_engine = WithFeedbackFilterNodes(\n",
    "    query_engine, feedback=f_context_relevance_score, threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruLlama(\n",
    "    filtered_query_engine,\n",
    "    app_name=\"LlamaIndex_App\",\n",
    "    app_version=\"filtered\",\n",
    "    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0363b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens.eval import Feedback, OpenAI as fOpenAI\n",
    "from trulens.feedback import Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb018cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show trulens\n",
    "!pip show llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import OpenAI as fOpenAI\n",
    "from trulens_eval.feedback import Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trulens.core\n",
    "import trulens.core.feedback\n",
    "import trulens.core.feedback.feedback\n",
    "dir(trulens.core.feedback.feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import FeedbackMode\n",
    "from trulens.core import Select\n",
    "from trulens.core import TruSession\n",
    "from trulens.apps.llamaindex import TruLlama\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "session = TruSession()\n",
    "\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c064fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./IPCC_AR6_WGII_Chapter03.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into a single large document rather than one document per-page\n",
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d05018",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown \"https://drive.google.com/uc?id=16pH4NETEs43dwJUvYnJ9Z-bsR9_krkrP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf sentence_index.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "sentence_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    node_parser=node_parser,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lparser",
   "language": "python",
   "name": "lparser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
